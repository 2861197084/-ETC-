# ETC 大数据分片设计方案

> 课程：大数据存储与管理 | 版本：v1.0

---

## 一、业务场景分析

### 1.1 核心业务数据特征

| 数据类型 | 数据量级 | 访问模式 | 时效性 | 存储选型 |
|----------|----------|----------|--------|----------|
| **卡口信息** | 19条（固定） | 只读为主 | 常驻 | MySQL（广播表） |
| **市内站点** | 百级 | 读多写少 | 常驻 | MySQL（广播表） |
| **通行记录** | 日增千万级 | 写多读少，批量查询 | 热7天，冷归档 | MySQL分片 + HBase |
| **卡口流量** | 实时流式 | 写多读多，实时统计 | 热1天 | Redis + Flink |
| **车辆信息** | 百万级 | 读多写少，点查为主 | 常驻热数据 | MySQL |
| **违规/套牌** | 日增万级 | 写少读少 | 中等热度 | MySQL |

### 1.2 业务场景划分

```
┌────────────────────────────────────────────────────────────────┐
│                     徐州市 ETC 大数据平台                        │
├────────────────────────────────────────────────────────────────┤
│                                                                │
│  ┌──────────────────────┐    ┌──────────────────────┐         │
│  │   出市卡口（19个）     │    │   市内站点（N个）     │         │
│  │   checkpoint 表       │    │   station 表         │         │
│  ├──────────────────────┤    ├──────────────────────┤         │
│  │ • 省际卡口 12个       │    │ • 主干道监测点        │         │
│  │   (苏皖界/苏鲁界)     │    │ • 交叉路口监测点      │         │
│  │ • 市际卡口 7个        │    │ • 高速出入口          │         │
│  │   (连云港/宿迁界)     │    │                      │         │
│  ├──────────────────────┤    ├──────────────────────┤         │
│  │ 核心功能：            │    │ 扩展功能：            │         │
│  │ ✓ 车流量预测（必做）  │    │ ✓ 路径规划           │         │
│  │ ✓ 出市车辆统计        │    │ ✓ 市内拥堵分析       │         │
│  │ ✓ 省际流量分析        │    │ ✓ 实时导航建议       │         │
│  └──────────────────────┘    └──────────────────────┘         │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

### 1.3 查询模式分析

```
高频查询：
├── 按车牌号查通行记录（点查）
├── 按卡口+时间段统计流量（范围查询）
├── 按时间范围查通行明细（范围扫描）
└── 实时流量排行榜（聚合查询）

低频查询：
├── 历史账单统计（冷数据聚合）
├── 套牌车轨迹回溯（全表扫描）
└── 跨月报表生成（批处理）
```

---

## 二、存储引擎选型

### 2.1 MySQL vs HBase 选型原则

| 特性 | MySQL | HBase |
|------|-------|-------|
| 数据模型 | 关系型，强Schema | 列族，灵活Schema |
| 事务支持 | ACID | 行级原子性 |
| 查询能力 | SQL，复杂JOIN | RowKey扫描，无JOIN |
| 扩展性 | 垂直扩展为主 | 水平扩展 |
| 适用场景 | 结构化、需要事务 | 海量、稀疏、时序 |

### 2.2 本项目存储分配

```
┌─────────────────────────────────────────────────────────┐
│                      MySQL 集群                          │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐      │
│  │  车辆信息    │  │  卡口信息    │  │  违规记录    │      │
│  │  (vehicle)  │  │ (checkpoint)│  │ (violation) │      │
│  └─────────────┘  └─────────────┘  └─────────────┘      │
│  理由：需要复杂查询、JOIN、事务保证                         │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│                      HBase 集群                          │
│  ┌─────────────────┐  ┌─────────────────┐               │
│  │   通行记录       │  │   车辆轨迹       │               │
│  │ (etc:pass_record)│ │(etc:trajectory) │               │
│  └─────────────────┘  └─────────────────┘               │
│  理由：海量写入、时序数据、范围扫描、无需JOIN               │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│                      Redis 缓存                          │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐      │
│  │ 实时流量统计 │  │  热点车辆    │  │  卡口状态    │      │
│  └─────────────┘  └─────────────┘  └─────────────┘      │
│  理由：高频访问、实时性要求、数据量小                       │
└─────────────────────────────────────────────────────────┘
```

---

## 三、分片策略设计

### 3.1 水平分片（Horizontal Sharding）

#### 3.1.1 通行记录表 - 按车牌哈希分片

**分片理由**：
- 通行记录是最大的表，日增千万条
- 最常见查询是「按车牌号查记录」
- 车牌分布相对均匀，适合哈希分片

**分片方案**：
```
分片键: plate_hash = hash(plate_number) % 4
分片数: 4 个分片

pass_record_0  ← plate_hash = 0
pass_record_1  ← plate_hash = 1  
pass_record_2  ← plate_hash = 2
pass_record_3  ← plate_hash = 3
```

**优势**：
- ✅ 按车牌查询只访问单分片，查询效率高
- ✅ 数据分布均匀，无热点问题
- ✅ 扩展时可以继续拆分

**劣势及应对**：
- ❌ 按时间范围查需要查所有分片 → 用 HBase 做时序查询
- ❌ 跨分片聚合复杂 → 通过 Flink 预聚合到统计表

#### 3.1.2 HBase 通行记录 - 按时间+卡口分片（预分区）

**RowKey 设计**：
```
RowKey = {salt(2位)}{yyyyMMdd}{checkpoint_id}{reverse_timestamp}

示例: 03_20251218_CP001_9999999999999
     ↑     ↑         ↑        ↑
   盐值  日期      卡口ID   反转时间戳
```

**设计理由**：
| 组成部分 | 作用 |
|----------|------|
| salt(2位) | 打散热点，避免写入集中到单个 Region |
| yyyyMMdd | 支持按天快速扫描 |
| checkpoint_id | 支持按卡口过滤 |
| reverse_timestamp | 最新数据排在前面，加速最近记录查询 |

**预分区策略**：
```shell
# 按盐值预分 16 个 Region
create 'etc:pass_record', 'cf',
  SPLITS => ['01', '02', '03', '04', '05', '06', '07', '08', 
             '09', '10', '11', '12', '13', '14', '15']
```

### 3.2 垂直分片（Vertical Sharding）

#### 3.2.1 车辆表垂直拆分

**拆分理由**：
- 车辆基础信息（车牌、类型）查询频繁
- 车主信息、ETC 绑定信息查询较少
- 拆分后减少单次查询的 IO

```sql
-- 主表：高频访问字段
vehicle_base (
  id, plate_number, plate_color, vehicle_type, brand, model
)

-- 扩展表：低频访问字段  
vehicle_ext (
  vehicle_id, owner_name, owner_phone, etc_card_no, bindTime
)
```

**查询优化**：
```sql
-- 通行记录页面：只查主表
SELECT * FROM vehicle_base WHERE plate_number = '京A12345';

-- 车辆详情页：JOIN 扩展表
SELECT b.*, e.owner_name, e.owner_phone 
FROM vehicle_base b 
LEFT JOIN vehicle_ext e ON b.id = e.vehicle_id
WHERE b.plate_number = '京A12345';
```

---

## 四、冷热数据分离

### 4.1 数据生命周期（新架构：Flink 双写）

```
                              ┌─────────────────────────────────────┐
                              │           Kafka Topic               │
                              │        etc-pass-records             │
                              └──────────────┬──────────────────────┘
                                             │
                                             ▼
                              ┌─────────────────────────────────────┐
                              │           Flink 实时计算             │
                              │  • 消费原始数据                      │
                              │  • 计算统计指标                      │
                              │  • 维护计数器                        │
                              └──────────────┬──────────────────────┘
                                             │
                    ┌────────────────────────┼────────────────────────┐
                    │                        │                        │
                    ▼                        ▼                        ▼
         ┌──────────────────┐    ┌──────────────────┐    ┌──────────────────┐
         │      Redis       │    │      MySQL       │    │      HBase       │
         │    (实时数据)     │    │   (7天温数据)    │    │   (全量历史)     │
         ├──────────────────┤    ├──────────────────┤    ├──────────────────┤
         │ • Flink计算结果   │    │ • 最近7天明细    │    │ • 全量通行记录   │
         │ • 车牌/卡口计数   │    │ • 索引快速查询   │    │ • 历史轨迹存储   │
         │ • 实时流量统计    │    │ • 复杂JOIN支持   │    │ • 范围扫描查询   │
         │ • 区域热度排名    │    │ • 定期清理7天前  │    │ • 永久保存       │
         └──────────────────┘    └──────────────────┘    └──────────────────┘
```

### 4.2 具体策略（更新）

| 数据类型 | Redis (实时) | MySQL (温数据) | HBase (全量) |
|----------|-------------|----------------|--------------|
| **通行记录** | ❌ 不存原始 | ✅ 最近7天 | ✅ 全量历史 |
| **流量统计** | ✅ Flink计算结果 | ❌ | ❌ |
| **车牌计数** | ✅ plate:count:xxx | ❌ | ❌ |
| **卡口计数** | ✅ cp:count:xxx | ❌ | ❌ |
| **违规记录** | ✅ 实时预警 | ✅ 待处理 | ✅ 已处理归档 |

### 4.3 数据同步任务

```
┌────────────────────────────────────────────────────────────────────────────┐
│                         Flink 实时双写任务                                  │
├────────────────────────────────────────────────────────────────────────────┤
│  DataSyncJob:                                                              │
│    Kafka → Flink → 同步写入 MySQL + HBase                                  │
│                                                                            │
│  CounterJob:                                                               │
│    每条记录 → Redis INCR plate:count:{plate} / cp:count:{cpId}            │
│                                                                            │
│  TrafficFlowJob:                                                           │
│    滚动窗口(1分钟) → 聚合 → Redis SET stats:flow:{cpId}:{minute}          │
├────────────────────────────────────────────────────────────────────────────┤
│                         Spark 离线清理任务                                  │
├────────────────────────────────────────────────────────────────────────────┤
│  每天凌晨: MySQL 删除 7天前的 pass_record 数据                              │
│  每月1号: 生成月度统计报表，写入 MySQL stats_monthly 表                     │
└────────────────────────────────────────────────────────────────────────────┘
```

### 4.4 Redis 计数器设计

```
┌─────────────────────────────────────────────────────────────────────┐
│                        Redis 计数器 Key 设计                         │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  车牌通行总数（永久）:                                               │
│    plate:count:{plateNumber}                                        │
│    例: plate:count:苏A12345 = 1234                                  │
│                                                                     │
│  车牌今日通行数（TTL 2天）:                                          │
│    plate:count:{date}:{plateNumber}                                 │
│    例: plate:count:20251220:苏A12345 = 5                            │
│                                                                     │
│  卡口通行总数（永久）:                                               │
│    cp:count:{checkpointId}                                          │
│    例: cp:count:CP001 = 9876543                                     │
│                                                                     │
│  卡口今日通行数（TTL 2天）:                                          │
│    cp:count:{date}:{checkpointId}                                   │
│    例: cp:count:20251220:CP001 = 2345                               │
│                                                                     │
│  存储估算:                                                          │
│    假设 3000万 不同车牌 × 32字节/key = 约 1GB                        │
│    + 19 卡口 × 32字节/key = 约 1KB                                  │
│    + 每日 key ×2 天 TTL = 约 200MB                                  │
│    总计 < 2GB，单节点 Redis 完全足够                                 │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 五、节点规划（7节点集群）

### 5.1 集群架构总览

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                            ETC 大数据集群架构                                 │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                        协调层 (Coordination)                         │    │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │    │
│  │  │ zookeeper-1 │  │ zookeeper-2 │  │ zookeeper-3 │                  │    │
│  │  │   (leader)  │  │  (follower) │  │  (follower) │                  │    │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                    │                                         │
│                                    ▼                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                        存储层 (Storage)                              │    │
│  │                                                                      │    │
│   MySQL 分片集群              HBase 集群               Redis       │    │
│  ┌───────┬───────┐      ┌───────────────┐         ┌─────────────┐  │    │
│  │mysql-1│mysql-2│      │  hbase-master │         │    redis    │  │    │
│  │ ds_0  │ ds_1  │      ├───────┬───────┤         │  (单节点)   │  │    │
│  │shard  │shard  │      │region1│region2│         │             │  │    │
│  │ 0,1   │ 2,3   │      │server │server │         │             │  │    │
│  └───────┴───────┘      └───────┴───────┘         └─────────────┘  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                    │                                         │
│                                    ▼                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                        计算层 (Compute)                              │    │
│  │  ┌─────────────────────────────────────────────────────────────┐   │    │
│  │  │                      Kafka + Flink                           │   │    │
│  │  │   kafka-broker  ←→  flink-jobmanager + taskmanager          │   │    │
│  │  └─────────────────────────────────────────────────────────────┘   │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

### 5.2 节点详细规划

| 节点编号 | 容器名 | 核心服务 | 端口映射 | 职责 |
|----------|--------|----------|----------|------|
| **Node-1** | mysql-1 | MySQL 8.0 | 3306 | 分片 ds_0，存储 shard_0, shard_1 |
| **Node-2** | mysql-2 | MySQL 8.0 | 3307 | 分片 ds_1，存储 shard_2, shard_3 |
| **Node-3** | hbase-master | HBase Master + RegionServer | 16010,16020 | HBase 主节点 + Region |
| **Node-4** | hbase-region | HBase RegionServer | 16030 | HBase Region 副本 |
| **Node-5** | redis | Redis 单节点 | 6379 | 计数器 + 实时统计 + 缓存 |
| **Node-6** | kafka-flink | Kafka + Flink | 9092,8081 | 消息队列 + 流计算 |
| **Node-7** | zookeeper | ZK 集群(3实例) | 2181,2182,2183 | 分布式协调 |

### 5.3 各节点数据分布

```
┌─────────────────────────────────────────────────────────────────────┐
│                         MySQL 分片分布                               │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   mysql-1 (ds_0)                    mysql-2 (ds_1)                  │
│   ┌─────────────────────┐           ┌─────────────────────┐        │
│   │ pass_record_0       │           │ pass_record_2       │        │
│   │ (plate_hash % 4 = 0)│           │ (plate_hash % 4 = 2)│        │
│   ├─────────────────────┤           ├─────────────────────┤        │
│   │ pass_record_1       │           │ pass_record_3       │        │
│   │ (plate_hash % 4 = 1)│           │ (plate_hash % 4 = 3)│        │
│   ├─────────────────────┤           ├─────────────────────┤        │
│   │ ===== 广播表 =====  │           │ ===== 广播表 =====  │        │
│   │ checkpoint (19条)   │           │ checkpoint (19条)   │        │
│   │ station (N条)       │           │ station (N条)       │        │
│   │ vehicle (全量)      │           │ vehicle (全量)      │        │
│   │ sys_user/sys_role   │           │ sys_user/sys_role   │        │
│   └─────────────────────┘           └─────────────────────┘        │
│                                                                     │
│   ShardingSphere 路由规则:                                          │
│   - pass_record: 按 plate_hash 分片到 ds_0/ds_1                     │
│   - checkpoint/station/vehicle: 广播表，两节点完整副本               │
│                                                                     │
│   广播表说明（不分片，全量复制）:                                     │
│   - checkpoint: 19个出市卡口，数据固定，查询频繁                      │
│   - station: 市内监测站点，数量有限（<100）                          │
│   - vehicle: 车辆信息，JOIN查询需要                                  │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│                         HBase Region 分布                           │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   hbase-master                      hbase-region                    │
│   ┌─────────────────────┐           ┌─────────────────────┐        │
│   │ etc:pass_record     │           │ etc:pass_record     │        │
│   │ Region: [00-07]     │           │ Region: [08-15]     │        │
│   │ (salt 00~07 的数据) │           │ (salt 08~15 的数据) │        │
│   ├─────────────────────┤           ├─────────────────────┤        │
│   │ etc:trajectory      │           │ etc:trajectory      │        │
│   │ Region: [0-7]       │           │ Region: [8-F]       │        │
│   └─────────────────────┘           └─────────────────────┘        │
│                                                                     │
│   RowKey 前缀决定 Region 归属:                                       │
│   - 00_20251218_xxx → hbase-master                                  │
│   - 09_20251218_xxx → hbase-region                                  │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│                         Redis 数据分布                              │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│   redis-master (:6379)              redis-slave (:6380)             │
│   ┌─────────────────────┐           ┌─────────────────────┐        │
│   │ 写入热数据:         │  ──复制→  │ 读取副本:           │        │
│   │ - stats:daily:*     │           │ - 同步 master 数据  │        │
│   │ - checkpoint:flow:* │           │ - 承担读请求        │        │
│   │ - vehicle:cache:*   │           │                     │        │
│   │ - alert:queue:*     │           │                     │        │
│   └─────────────────────┘           └─────────────────────┘        │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### 5.4 Docker Compose 网络规划

```yaml
networks:
  etc-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# 各容器 IP 分配
# 172.20.0.10 - mysql-1
# 172.20.0.11 - mysql-2  
# 172.20.0.20 - hbase-master
# 172.20.0.21 - hbase-region
# 172.20.0.30 - redis-master
# 172.20.0.31 - redis-slave
# 172.20.0.40 - kafka
# 172.20.0.41 - flink
# 172.20.0.50/51/52 - zookeeper 集群
```

---

## 六、查询路由策略

### 6.1 读写分离

```java
// 伪代码：查询路由
public List<PassRecord> queryRecords(String plateNumber, Date startTime) {
    
    // 1. 判断是否热数据查询
    if (isToday(startTime)) {
        return redisService.getTodayRecords(plateNumber);
    }
    
    // 2. 判断是否温数据（7天内）
    // 2. 温数据（7天内）走 MySQL 分片
    int shard = hash(plateNumber) % 4;
    List<PassRecord> mysqlRecords = mysqlShardService.query(shard, plateNumber, 7);
    
    // 3. 从 Redis 获取总数（毫秒级）
    Long totalCount = redisService.get("plate:count:" + plateNumber);
    
    // 4. 返回结果（包含总数和最近7天数据）
    return QueryResult.builder()
        .records(mysqlRecords)
        .totalCount(totalCount)
        .hasMore(totalCount > mysqlRecords.size())
        .build();
}

/**
 * 加载更多历史记录（用户点击"展示更多"时调用）
 */
public List<PassRecord> loadMoreHistory(String plateNumber, String startTime) {
    // 历史数据从 HBase 查询
    String rowKeyPrefix = buildRowKeyPrefix(startTime, plateNumber);
    return hbaseService.scanByPrefix(rowKeyPrefix);
}
```

### 6.2 聚合查询优化

```
场景：统计某卡口今日车流量

传统方式（慢）：
SELECT COUNT(*) FROM pass_record WHERE checkpoint_id = 'CP001' AND date = today
→ 需要扫描所有分片

优化方式（快）：
1. Flink 消费 Kafka 时实时更新 Redis 计数器
2. 查询直接读 Redis: GET cp:count:20251218:CP001
→ O(1) 复杂度
```

### 6.3 渐进式查询策略（新）

```
┌─────────────────────────────────────────────────────────────────────────┐
│                          渐进式查询流程                                   │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  用户请求: 查询车牌 苏A12345 的通行记录                                    │
│                         │                                               │
│                         ▼                                               │
│              ┌─────────────────────┐                                    │
│              │   后端并行请求        │                                    │
│              └──────────┬──────────┘                                    │
│                         │                                               │
│        ┌────────────────┼────────────────┐                              │
│        │                │                │                              │
│        ▼                ▼                ▼                              │
│   ┌─────────┐     ┌─────────┐     ┌─────────────┐                       │
│   │  Redis  │     │  MySQL  │     │   HBase     │                       │
│   │ 获取总数 │     │ 最近7天 │     │ 后台预热     │                       │
│   └────┬────┘     └────┬────┘     └──────┬──────┘                       │
│        │ 1ms          │ 20ms            │ (不阻塞)                       │
│        │               │                │                               │
│        └───────┬───────┘                │                               │
│                │                        │                               │
│                ▼                        │                               │
│   ┌──────────────────────────────┐      │                               │
│   │ 前端先展示:                   │      │                               │
│   │ • 总通行次数: 1,234 次        │      │                               │
│   │ • 最近7天记录列表             │      │                               │
│   │ • [展示更多历史记录] 按钮     │      │                               │
│   └──────────────────────────────┘      │                               │
│                                         │                               │
│   用户点击"展示更多"                     │                               │
│                │                        │                               │
│                └────────────────────────┘                               │
│                                         │                               │
│                                         ▼                               │
│                          ┌──────────────────────────────┐               │
│                          │ 加载历史数据（此时HBase已预热）│               │
│                          │ 显示全部历史通行记录          │               │
│                          └──────────────────────────────┘               │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 七、设计亮点总结（答辩要点）

### 7.1 体现的大数据知识点

| 知识点 | 在本项目中的应用 |
|--------|-----------------|
| **水平分片** | 通行记录按车牌哈希分 4 片，解决单表过大问题 |
| **垂直分片** | 车辆表拆分为 base/ext，减少查询 IO |
| **冷热分离** | Flink 双写：Redis(实时)+MySQL(7天)+HBase(全量历史) |
| **读写分离** | MySQL 主从，写主读从 |
| **渐进式加载** | 先展示 MySQL 快速结果，再加载 HBase 历史 |
| **RowKey设计** | 盐值+时间+ID，避免热点+支持范围扫描 |
| **预分区** | HBase 预分 16 Region，避免写入热点 |

### 7.2 为什么这样设计

```
Q: 为什么通行记录用车牌哈希分片而不是时间分片？
A: 最频繁的查询是"查某车牌的通行记录"，按车牌分片可以精确路由到单个分片，
   避免查询时扫描所有分片。时间范围查询通过 HBase 解决。

Q: 为什么需要 MySQL 和 HBase 两套存储？
A: MySQL 适合需要 JOIN、事务的场景（如车辆-车主关联查询）
   HBase 适合海量时序数据的范围扫描（如查历史轨迹）
   Flink 同时双写两者，MySQL 保留7天用于快速查询，HBase 保留全量用于历史追溯。
   两者互补，各取所长。

Q: Redis 在架构中的作用？
A: 1. 存储 Flink 实时计算结果（流量统计、车源分布、区域热度等）
   2. 维护车牌/卡口通行计数器（解决 HBase COUNT 慢的问题）
   3. 缓存热点数据（会话、配置等）
   单节点 Redis 足以支撑30M车牌×8字节≈240MB 计数器存储。

Q: 3 个节点够用吗？
A: 对于课程演示足够。真实生产环境会根据数据量水平扩展。
   设计上预留了扩展能力（分片数可增加、Region 可分裂）。
```

## 八、Docker Compose 配置

### 8.1 完整 docker-compose.yml

```yaml
version: '3.8'

services:
  # ==================== ZooKeeper 集群 ====================
  zookeeper-1:
    image: zookeeper:3.9
    container_name: zk-1
    hostname: zk-1
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zk-1:2888:3888;2181 server.2=zk-2:2888:3888;2181 server.3=zk-3:2888:3888;2181
    networks:
      etc-net:
        ipv4_address: 172.20.0.50

  zookeeper-2:
    image: zookeeper:3.9
    container_name: zk-2
    hostname: zk-2
    ports:
      - "2182:2181"
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zk-1:2888:3888;2181 server.2=zk-2:2888:3888;2181 server.3=zk-3:2888:3888;2181
    networks:
      etc-net:
        ipv4_address: 172.20.0.51

  zookeeper-3:
    image: zookeeper:3.9
    container_name: zk-3
    hostname: zk-3
    ports:
      - "2183:2181"
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zk-1:2888:3888;2181 server.2=zk-2:2888:3888;2181 server.3=zk-3:2888:3888;2181
    networks:
      etc-net:
        ipv4_address: 172.20.0.52

  # ==================== MySQL 分片集群 ====================
  mysql-1:
    image: mysql:8.0
    container_name: mysql-1
    hostname: mysql-1
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: etc_ds0
    volumes:
      - ./mysql/init-ds0.sql:/docker-entrypoint-initdb.d/init.sql
      - mysql1-data:/var/lib/mysql
    command: --default-authentication-plugin=mysql_native_password
    networks:
      etc-net:
        ipv4_address: 172.20.0.10

  mysql-2:
    image: mysql:8.0
    container_name: mysql-2
    hostname: mysql-2
    ports:
      - "3307:3306"
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: etc_ds1
    volumes:
      - ./mysql/init-ds1.sql:/docker-entrypoint-initdb.d/init.sql
      - mysql2-data:/var/lib/mysql
    command: --default-authentication-plugin=mysql_native_password
    networks:
      etc-net:
        ipv4_address: 172.20.0.11

  # ==================== ShardingSphere Proxy ====================
  shardingsphere-proxy:
    image: apache/shardingsphere-proxy:5.4.1
    container_name: sharding-proxy
    hostname: sharding-proxy
    ports:
      - "3308:3307"
    volumes:
      - ./shardingsphere/config:/opt/shardingsphere-proxy/conf
    depends_on:
      - mysql-1
      - mysql-2
    networks:
      etc-net:
        ipv4_address: 172.20.0.12

  # ==================== HBase 集群 ====================
  hbase-master:
    image: harisekhon/hbase:2.1
    container_name: hbase-master
    hostname: hbase-master
    ports:
      - "16010:16010"  # Master Web UI
      - "16020:16020"  # Master RPC
      - "9090:9090"    # Thrift
    environment:
      HBASE_MANAGES_ZK: "false"
    volumes:
      - ./hbase/hbase-site.xml:/hbase/conf/hbase-site.xml
      - hbase-master-data:/hbase-data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    networks:
      etc-net:
        ipv4_address: 172.20.0.20

  hbase-region:
    image: harisekhon/hbase:2.1
    container_name: hbase-region
    hostname: hbase-region
    ports:
      - "16030:16030"  # RegionServer Web UI
    environment:
      HBASE_MANAGES_ZK: "false"
    volumes:
      - ./hbase/hbase-site.xml:/hbase/conf/hbase-site.xml
      - hbase-region-data:/hbase-data
    depends_on:
      - hbase-master
    command: regionserver
    networks:
      etc-net:
        ipv4_address: 172.20.0.21

  # ==================== Redis 单节点 ====================
  redis:
    image: redis:7-alpine
    container_name: redis
    hostname: redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    networks:
      etc-net:
        ipv4_address: 172.20.0.30

  # ==================== Kafka ====================
  kafka:
    image: bitnami/kafka:3.6
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_CFG_ZOOKEEPER_CONNECT: zk-1:2181,zk-2:2181,zk-3:2181
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    networks:
      etc-net:
        ipv4_address: 172.20.0.40

  # ==================== Flink ====================
  flink-jobmanager:
    image: flink:1.18-java11
    container_name: flink-jobmanager
    hostname: flink-jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
    networks:
      etc-net:
        ipv4_address: 172.20.0.41

  flink-taskmanager:
    image: flink:1.18-java11
    container_name: flink-taskmanager
    hostname: flink-taskmanager
    command: taskmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
    depends_on:
      - flink-jobmanager
    networks:
      etc-net:
        ipv4_address: 172.20.0.42

networks:
  etc-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  mysql1-data:
  mysql2-data:
  hbase-master-data:
  hbase-region-data:
  redis-data:
```

### 8.2 ShardingSphere 分片配置

```yaml
# shardingsphere/config/config-sharding.yaml
dataSources:
  ds_0:
    dataSourceClassName: com.zaxxer.hikari.HikariDataSource
    driverClassName: com.mysql.cj.jdbc.Driver
    jdbcUrl: jdbc:mysql://mysql-1:3306/etc_ds0?useSSL=false&allowPublicKeyRetrieval=true
    username: root
    password: root123
  ds_1:
    dataSourceClassName: com.zaxxer.hikari.HikariDataSource
    driverClassName: com.mysql.cj.jdbc.Driver
    jdbcUrl: jdbc:mysql://mysql-2:3306/etc_ds1?useSSL=false&allowPublicKeyRetrieval=true
    username: root
    password: root123

rules:
  - !SHARDING
    tables:
      # 通行记录 - 按车牌哈希分片
      pass_record:
        actualDataNodes: ds_${0..1}.pass_record_${0..3}
        databaseStrategy:
          standard:
            shardingColumn: plate_hash
            shardingAlgorithmName: db_mod
        tableStrategy:
          standard:
            shardingColumn: plate_hash
            shardingAlgorithmName: table_mod
        keyGenerateStrategy:
          column: id
          keyGeneratorName: snowflake
          
      # 违规记录 - 按车牌哈希分片
      violation:
        actualDataNodes: ds_${0..1}.violation_${0..1}
        databaseStrategy:
          standard:
            shardingColumn: plate_hash
            shardingAlgorithmName: db_mod
            
    # 广播表 - 所有节点完整副本（数据量小、查询频繁）
    broadcastTables:
      - vehicle          # 车辆信息
      - checkpoint       # 19个出市卡口（固定）
      - station          # 市内监测站点
      - sys_user         # 系统用户
      - sys_role         # 角色
      
    shardingAlgorithms:
      db_mod:
        type: MOD
        props:
          sharding-count: 2
      table_mod:
        type: MOD
        props:
          sharding-count: 4
          
    keyGenerators:
      snowflake:
        type: SNOWFLAKE
```

### 8.3 HBase 配置

```xml
<!-- hbase/hbase-site.xml -->
<configuration>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>zk-1,zk-2,zk-3</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.clientPort</name>
    <value>2181</value>
  </property>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///hbase-data</value>
  </property>
</configuration>
```

### 8.4 启动脚本

```bash
#!/bin/bash
# start-cluster.sh

echo "1. 启动 ZooKeeper 集群..."
docker-compose up -d zookeeper-1 zookeeper-2 zookeeper-3
sleep 10

echo "2. 启动 MySQL 分片节点..."
docker-compose up -d mysql-1 mysql-2
sleep 15

echo "3. 启动 ShardingSphere Proxy..."
docker-compose up -d shardingsphere-proxy
sleep 10

echo "4. 启动 HBase 集群..."
docker-compose up -d hbase-master
sleep 10
docker-compose up -d hbase-region
sleep 5

echo "5. 启动 Redis..."
docker-compose up -d redis
sleep 5

echo "6. 启动 Kafka..."
docker-compose up -d kafka
sleep 10

echo "7. 启动 Flink..."
docker-compose up -d flink-jobmanager flink-taskmanager

echo "集群启动完成！"
echo "访问地址："
echo "  - ShardingSphere: localhost:3308"
echo "  - HBase UI: http://localhost:16010"
echo "  - Flink UI: http://localhost:8081"
```

---

## 九、架构变更说明（v2.0 更新）

### 9.1 架构变更对比

| 项目 | v1.0 (原方案) | v2.0 (新方案) |
|------|--------------|--------------|
| **数据流** | Redis热→MySQL温→HBase冷 | Flink双写：MySQL+HBase同时 |
| **Redis定位** | 热数据临时存储 | 计算结果+计数器存储 |
| **Redis部署** | 主从集群 | 单节点(2GB足够) |
| **HBase数据** | 仅7天后冷数据 | 全量历史数据 |
| **MySQL清理** | 自动迁移到HBase | 定时删除7天前 |
| **查询策略** | 按时间分层查询 | 渐进式加载(MySQL先→HBase后) |

### 9.2 变更理由

1. **简化数据流**：原方案需要定时迁移任务，增加运维复杂度
2. **提升查询体验**：渐进式加载让用户先看到快速结果
3. **解决HBase COUNT慢**：Redis计数器毫秒级返回总数
4. **降低资源需求**：单节点Redis足以支撑，减少运维成本

### 9.3 待完成开发

详见 [待完善功能.md](./待完善功能.md)

---

*最后更新：2025-12-20 | 版本：v2.0*
