name: etc-platform

services:
  zookeeper:
    image: zookeeper:3.9
    environment:
      ZOO_4LW_COMMANDS_WHITELIST: ruok,stat,mntr
      ZOO_TICK_TIME: 2000
      ZOO_INIT_LIMIT: 10
      ZOO_SYNC_LIMIT: 5
    ports:
      - "2181:2181"
    volumes:
      - zookeeper-data:/data
      - zookeeper-datalog:/datalog
    healthcheck:
      test: [ "CMD-SHELL", "echo ruok | nc 127.0.0.1 2181 | grep -q imok" ]
      interval: 10s
      timeout: 5s
      retries: 12

  kafka:
    build:
      context: ./infra/kafka
      args:
        KAFKA_VERSION: "3.8.1"
        SCALA_VERSION: "2.13"
    # 开发环境优先保证可启动（Docker volume 首次创建时常为 root:root，非 root 容器用户会写入失败）
    user: "0:0"
    environment:
      KAFKA_NODE_ID: "1"
      KAFKA_CLUSTER_ID: "${KAFKA_CLUSTER_ID:-}"
      KAFKA_EXTERNAL_HOST: "${KAFKA_EXTERNAL_HOST:-localhost}"
    ports:
      - "19092:19092" # host access (EXTERNAL)
    volumes:
      - kafka-data:/var/lib/kafka
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server 127.0.0.1:9092 >/dev/null 2>&1" ]
      interval: 10s
      timeout: 10s
      retries: 18

  kafka-init:
    build:
      context: ./infra/kafka
      args:
        KAFKA_VERSION: "3.8.1"
        SCALA_VERSION: "2.13"
    entrypoint: [ "bash", "-lc" ]
    user: "0:0"
    depends_on:
      kafka:
        condition: service_healthy
    command:
      - "/opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic etc-pass-records --partitions 6 --replication-factor 1 && /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic etc.pass.clean --partitions 6 --replication-factor 1 && /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic etc.alerts.clone_plate --partitions 3 --replication-factor 1 && /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic etc.alerts.pressure --partitions 3 --replication-factor 1 && echo 'kafka topics ensured.'"
    restart: "no"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: [ "redis-server", "--appendonly", "yes" ]
    volumes:
      - redis-data:/data
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 12

  mysql0:
    image: mysql:8.4
    environment:
      MYSQL_ROOT_PASSWORD: "${MYSQL_ROOT_PASSWORD:-root}"
      TZ: Asia/Shanghai
    ports:
      - "33070:3306"
    volumes:
      - mysql0-data:/var/lib/mysql
      - ./infra/mysql/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: [ "CMD-SHELL", "mysqladmin ping -h 127.0.0.1 -uroot -p$$MYSQL_ROOT_PASSWORD --silent" ]
      interval: 10s
      timeout: 5s
      retries: 18

  mysql1:
    image: mysql:8.4
    environment:
      MYSQL_ROOT_PASSWORD: "${MYSQL_ROOT_PASSWORD:-root}"
      TZ: Asia/Shanghai
    ports:
      - "33061:3306"
    volumes:
      - mysql1-data:/var/lib/mysql
      - ./infra/mysql/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: [ "CMD-SHELL", "mysqladmin ping -h 127.0.0.1 -uroot -p$$MYSQL_ROOT_PASSWORD --silent" ]
      interval: 10s
      timeout: 5s
      retries: 18

  shardingsphere:
    image: apache/shardingsphere-proxy:5.4.1
    depends_on:
      mysql0:
        condition: service_healthy
      mysql1:
        condition: service_healthy
    ports:
      - "3307:3307"
    volumes:
      - ./infra/shardingsphere/conf:/opt/shardingsphere-proxy/conf
      - ./infra/shardingsphere/ext-lib:/opt/shardingsphere-proxy/ext-lib
    environment:
      JAVA_OPTS: "-Xms512m -Xmx1024m"
    healthcheck:
      test: [ "CMD-SHELL", "bash -lc 'exec 3<>/dev/tcp/127.0.0.1/3307'" ]
      interval: 10s
      timeout: 5s
      retries: 18

  flink-jobmanager:
    build:
      context: ./infra/flink
      args:
        FLINK_VERSION: "1.20.0"
        KAFKA_CONNECTOR_VERSION: "3.4.0-1.20"
    command: jobmanager
    ports:
      - "8081:8081"
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.rpc.port: 6123
        rest.address: flink-jobmanager
        rest.port: 8081
        blob.server.port: 6124
        taskmanager.numberOfTaskSlots: 3
        parallelism.default: 1
        jobmanager.memory.process.size: 1024m
        state.backend: hashmap
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints
        execution.checkpointing.interval: 60000
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./flink-jobs/target:/opt/flink/jobs:ro
      - flink-checkpoints:/tmp/flink-checkpoints
      - flink-savepoints:/tmp/flink-savepoints

  flink-taskmanager:
    build:
      context: ./infra/flink
      args:
        FLINK_VERSION: "1.20.0"
        KAFKA_CONNECTOR_VERSION: "3.4.0-1.20"
    command: taskmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.rpc.port: 6123
        blob.server.port: 6124
        taskmanager.numberOfTaskSlots: 3
        parallelism.default: 1
        taskmanager.memory.process.size: 2048m
        state.backend: hashmap
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints
    depends_on:
      - flink-jobmanager
    volumes:
      - flink-checkpoints:/tmp/flink-checkpoints
      - flink-savepoints:/tmp/flink-savepoints

  hbase:
    build:
      context: ./infra/hbase
      args:
        HBASE_VERSION: "2.6.1"
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "16010:16010" # HBase Master UI
      - "16030:16030" # RegionServer UI
      - "9090:9090" # Thrift Server
    volumes:
      - hbase-data:/data
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://127.0.0.1:16010/master-status >/dev/null 2>&1" ]
      interval: 15s
      timeout: 10s
      retries: 20

  hbase-init:
    build:
      context: ./infra/hbase
      args:
        HBASE_VERSION: "2.6.1"
    depends_on:
      hbase:
        condition: service_healthy
    entrypoint: [ "bash", "-c" ]
    command: [ "sed -i 's/\\r$//' /init/init-tables.sh && chmod +x /init/init-tables.sh && /init/init-tables.sh" ]
    restart: "no"

  # ==================== Presto/Trino - SQL 查询引擎 ====================
  trino:
    image: trinodb/trino:439
    container_name: trino
    hostname: trino
    ports:
      - "8090:8080" # Trino Web UI & Query Endpoint
    volumes:
      - ./infra/trino/etc/config.properties:/etc/trino/config.properties:ro
      - ./infra/trino/etc/node.properties:/etc/trino/node.properties:ro
      - ./infra/trino/etc/jvm.config:/etc/trino/jvm.config:ro
      - ./infra/trino/etc/log.properties:/etc/trino/log.properties:ro
      - ./infra/trino/catalog:/etc/trino/catalog:ro
    depends_on:
      mysql0:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://localhost:8080/v1/status | grep -q ACTIVE" ]
      interval: 10s
      timeout: 5s
      retries: 18

  spark-master:
    build:
      context: ./infra/spark
      args:
        SPARK_VERSION: "3.5.4"
    environment:
      SPARK_MODE: master
      TZ: Asia/Shanghai
    ports:
      - "18080:8080" # Spark master UI (avoid conflict with backend:8080)
      - "7077:7077" # Spark master
    volumes:
      - spark-data:/data
    restart: unless-stopped

  spark-worker:
    build:
      context: ./infra/spark
      args:
        SPARK_VERSION: "3.5.4"
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      TZ: Asia/Shanghai
    depends_on:
      - spark-master
    ports:
      - "8082:8081" # Spark worker UI
    volumes:
      - spark-data:/data
    restart: unless-stopped

  # ==================== Spark 推理作业（Time-MoE 预测） ====================
  # ⚠️ 默认不启动：容器内安装 PyTorch/Transformers 体积巨大，构建耗时很长。
  # 推荐方案：在本机运行预测轮询进程（见 scripts/run-forecast-local.py），
  # 仍然会读写同一套 MySQL 表（forecast_request / checkpoint_flow_forecast_5m）。
  spark-forecast:
    build:
      context: ./infra/spark
      args:
        SPARK_VERSION: "3.5.4"
    profiles: [ "docker-forecast" ]
    depends_on:
      spark-master:
        condition: service_started
      shardingsphere:
        condition: service_healthy
    environment:
      TZ: Asia/Shanghai
      SPARK_MASTER_URL: spark://spark-master:7077
      MYSQL_HOST: shardingsphere
      MYSQL_PORT: 3307
      MYSQL_USER: root
      MYSQL_PASSWORD: "${MYSQL_ROOT_PASSWORD:-root}"
      MYSQL_DB: etc
      MODEL_DIR: /workspace/model/etc_flow_5min_epoch5
      MODEL_VERSION: timemoe_etc_flow_v1
      LOOP_SECONDS: "10"
      TORCH_DEVICE: cpu
      TORCH_DTYPE: fp32
    entrypoint: [ "bash", "-lc" ]
    command:
      - "spark-submit --master ${SPARK_MASTER_URL} /workspace/spark-jobs/etc_flow_forecast_job.py --loop_seconds ${LOOP_SECONDS}"
    volumes:
      - ./spark-jobs:/workspace/spark-jobs:ro
      - ./Time-MoE:/workspace/Time-MoE:ro
      - ./model:/workspace/model:ro
    restart: unless-stopped

  # ==================== 后端 API 服务 ====================
  backend:
    build:
      context: ./backend
    ports:
      - "8080:8080"
    environment:
      # Ensure MySQL DATETIME values are rendered consistently with the demo's simulated timezone.
      - JAVA_TOOL_OPTIONS=-Duser.timezone=Asia/Shanghai
      - MYSQL_HOST=shardingsphere
      - MYSQL_PORT=3307
      - MYSQL_USER=root
      - MYSQL_PASSWORD=${MYSQL_ROOT_PASSWORD:-root}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      # DashScope API Key for Agent and TTS：仅透传宿主机环境变量（未设置则不注入，避免覆盖后端默认配置）
      - DASHSCOPE_API_KEY
    depends_on:
      shardingsphere:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://localhost:8080/actuator/health || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # ==================== Python 数据服务 ====================
  data-service:
    build:
      context: ./data-service
    environment:
      MYSQL_HOST: shardingsphere
      MYSQL_PORT: 3307
      MYSQL_USER: root
      MYSQL_PASSWORD: "${MYSQL_ROOT_PASSWORD:-root}"
      REDIS_HOST: redis
      REDIS_PORT: 6379
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      DATA_DIR: /data
      BACKEND_TIME_API: http://backend:8080/api/time
    volumes:
      - ./data/expanded:/data:ro
    depends_on:
      mysql0:
        condition: service_healthy
      kafka:
        condition: service_healthy
    restart: unless-stopped

  # ==================== Demo 数据预热（可选） ====================
  # 说明：用于“预测分析页”演示时防止 MySQL 热数据为空导致曲线全 0。
  # 幂等：仅当 pass_record 为空时才会生成（避免重复灌数据）。
  # 启动时自动预热：生成最近 24 小时少量历史数据（写入 ShardingSphere -> MySQL）。
  data-seed:
    build:
      context: ./data-service
    depends_on:
      shardingsphere:
        condition: service_healthy
    environment:
      MYSQL_HOST: shardingsphere
      MYSQL_PORT: 3307
      MYSQL_USER: root
      MYSQL_PASSWORD: "${MYSQL_ROOT_PASSWORD:-root}"
      # HBase 非必需：没启 profile hbase 时会自动只写 MySQL
      HBASE_THRIFT_HOST: hbase
      HBASE_THRIFT_PORT: 9090
    entrypoint: [ "bash", "-lc" ]
    command:
      - "python -m scripts.generate_history --hours 24 --rate 2 --only-if-empty"
    restart: "no"

  # ==================== Vanna Text2SQL 服务 ====================
  vanna-service:
    build:
      context: ./vanna-service
    profiles: [ "vanna" ]
    ports:
      - "8100:8100"
    environment:
      # LLM 配置
      LLM_PROVIDER: "${LLM_PROVIDER:-openai}"
      OPENAI_API_KEY: "${OPENAI_API_KEY:-}"
      OPENAI_MODEL: "${OPENAI_MODEL:-gpt-4o-mini}"
      OLLAMA_HOST: "${OLLAMA_HOST:-http://host.docker.internal:11434}"
      OLLAMA_MODEL: "${OLLAMA_MODEL:-qwen2.5:7b}"
      # MySQL 配置
      MYSQL_HOST: shardingsphere
      MYSQL_PORT: 3307
      MYSQL_USER: root
      MYSQL_PASSWORD: "${MYSQL_ROOT_PASSWORD:-root}"
      MYSQL_DATABASE: etc
      # 服务配置
      CHROMA_PATH: /app/chroma_data
      VANNA_HOST: "0.0.0.0"
      VANNA_PORT: "8100"
    volumes:
      - vanna-chroma:/app/chroma_data
    depends_on:
      shardingsphere:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://localhost:8100/health || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

volumes:
  zookeeper-data:
  zookeeper-datalog:
  kafka-data:
  redis-data:
  mysql0-data:
  mysql1-data:
  hbase-data:
  spark-data:
  trino-data:
  flink-checkpoints:
  flink-savepoints:
  vanna-chroma:
